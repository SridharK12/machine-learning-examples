{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag,ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine learning is the science of getting computers to act without being explicitly programmed.',\n",
       " 'In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome.',\n",
       " 'Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it.',\n",
       " 'Many researchers also think it is the best way to make progress towards human-level AI.',\n",
       " 'In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself.',\n",
       " \"More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems.\",\n",
       " \"Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'the',\n",
       " 'science',\n",
       " 'of',\n",
       " 'getting',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'act']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "postags = pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'NN'),\n",
       " ('learning', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('science', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('getting', 'VBG'),\n",
       " ('computers', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('act', 'VB'),\n",
       " ('without', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('explicitly', 'RB'),\n",
       " ('programmed', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('past', 'JJ'),\n",
       " ('decade', 'NN'),\n",
       " (',', ',')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postags[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"Google Incorporation Limited is starting a new office at Hyderabad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = word_tokenize(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 'NNP'),\n",
       " ('Incorporation', 'NNP'),\n",
       " ('Limited', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('starting', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('new', 'JJ'),\n",
       " ('office', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('Hyderabad', 'NNP')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " pos_tag(word1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Manoj',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'boy',\n",
       " '.',\n",
       " 'He',\n",
       " 'is',\n",
       " 'playful',\n",
       " '.',\n",
       " 'He',\n",
       " 'is',\n",
       " 'good',\n",
       " 'in',\n",
       " 'Science',\n",
       " ',',\n",
       " 'Social',\n",
       " 'studies',\n",
       " '.',\n",
       " 'He',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'in',\n",
       " 'Mathematics',\n",
       " '.',\n",
       " 'Apart',\n",
       " 'from',\n",
       " 'academics',\n",
       " ',',\n",
       " 'Manoj',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'chess',\n",
       " 'player',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2=\"Manoj is a good boy. He is playful. He is good in Science, Social studies. He needs to improve in Mathematics. Apart from academics, Manoj is a good chess player.\"\n",
    "sentences=sent_tokenize(text2)\n",
    "sentences\n",
    "words=word_tokenize(text2)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ner'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5a634d3dca7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mner\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ner'"
     ]
    }
   ],
   "source": [
    "import ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" google chief is visiting india during the month of may, they are going to setup a new R&D facility at hyderabad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Dell\\\\Python'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"Mark and John are working at Google.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  and/CC\n",
      "  (PERSON John/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(pos_tag(word_tokenize(text3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4='I went to Sridhar''s house'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S I/PRP went/VBD to/TO (GPE Sridhars/NNP) house/NN)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(pos_tag(word_tokenize(text4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S What/WP weather/NN (GPE Chicago/NNP) today/NN ?/.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import load_parser\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = \"What is the weather in Chicago today? \"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "clean_tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "tagged = nltk.pos_tag(clean_tokens)\n",
    "\n",
    "print(nltk.ne_chunk(tagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "            sample = \"hello my name is Shefali and I live in Nebraska.\"\n",
    "            print(sample)\n",
    "            sentences = nltk.sent_tokenize(sample)\n",
    "            tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "            tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "            chunked_sentences = nltk.ne_chunk_sents(tagged_sentences)\n",
    "\n",
    "            print(list(chunked_sentences))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello my name is Shefali and I live in Nebraska.\n",
      "[Tree('S', [('hello', 'NN'), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), Tree('PERSON', [('Shefali', 'NNP')]), ('and', 'CC'), ('I', 'PRP'), ('live', 'VBP'), ('in', 'IN'), Tree('GPE', [('Nebraska', 'NNP')]), ('.', '.')])]\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "with open('sample.txt', 'r') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "\n",
    "sentences = nltk.sent_tokenize(sample)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "entity_names = []\n",
    "for tree in chunked_sentences:\n",
    "    # Print results per sentence\n",
    "    # print extract_entity_names(tree)\n",
    "\n",
    "    entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "# Print all entity names\n",
    "#print entity_names\n",
    "\n",
    "# Print unique entity names\n",
    "print set(entity_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sentences:\n",
    "    print sent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
